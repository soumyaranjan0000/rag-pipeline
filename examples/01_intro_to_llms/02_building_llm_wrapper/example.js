/**
 * BUILDING AN LLM WRAPPER CLASS
 *
 * Now that we understand how node-llama-cpp works, let's build
 * a cleaner wrapper class that follows common patterns.
 *
 * This example builds the wrapper step-by-step with explanations.
 */


import {LlamaCpp} from "../../../src/llms/index.js";

console.log('ğŸ—ï¸  Building an LLM Wrapper Class\n');

console.log(`
WHY BUILD A WRAPPER?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Simpler API
   - Hide node-llama-cpp complexity
   - Consistent interface
   - Easy to swap implementations

2. Better Defaults
   - Pre-configured settings
   - Common use case patterns
   - Sensible parameter choices

3. Reusability
   - Use across your RAG system
   - Easy to test and mock
   - Follows standard patterns

4. Future-Proof
   - Easy to add features
   - Can switch backends
   - Maintainable code

Let's build it step by step!
`);

// ============================================================================
// DEMO: Using our wrapper
// ============================================================================

console.log('\nğŸ“ EXAMPLE 1: Basic Usage\n');

const model = await LlamaCpp.initialize({
    modelPath: process.env.MODEL_PATH || './models/llama-3.1-8b-q4_0.gguf',
    maxTokens: 100,
    temperature: 0.7,
});

const response1 = await model.invoke(
    'What are the three primary colors?'
);
console.log('Q: What are the three primary colors?');
console.log('A:', response1);

console.log('\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n');
console.log('ğŸ“ EXAMPLE 2: Streaming Tokens\n');

console.log('Q: Count from 1 to 5.');
console.log('A: ');

for await (const chunk of model.stream('Count from 1 to 5.')) {
    process.stdout.write(chunk);
}

console.log('\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n');
console.log('ğŸ“ EXAMPLE 3: Batch Processing\n');

const questions = [
    'What is 2+2?',
    'What color is the sky?',
    'What is the capital of France?',
];

console.log('Processing multiple questions...\n');
const responses = await model.batch(questions);

questions.forEach((q, i) => {
    console.log(`Q${i + 1}: ${q}`);
    console.log(`A${i + 1}: ${responses[i]}\n`);
});

console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n');
console.log('ğŸ“ EXAMPLE 4: Custom Options\n');

const response2 = await model.invoke(
    'Write a creative story opening.',
    {
        maxTokens: 150,
        temperature: 0.9,  // More creative
    }
);

console.log('Q: Write a creative story opening.');
console.log('A:', response2);

// Cleanup
await model.dispose();

console.log(`

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    WRAPPER BENEFITS                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Clean, simple API
âœ“ Easy to understand and use
âœ“ Consistent with other frameworks
âœ“ Handles resource management
âœ“ Supports streaming and batching
âœ“ Configurable but with good defaults

WHAT WE BUILT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. BaseLLM (BaseLLM.js)
   - Abstract base class
   - Defines the interface all LLMs must follow
   - Common methods like batch()

2. LlamaCpp (LlamaCpp.js)
   - Concrete implementation for node-llama-cpp
   - Handles model loading and management
   - Provides invoke(), stream(), batch()

3. Benefits for RAG:
   - Easy to use in retrieval pipelines
   - Can swap for API-based models later
   - Clean separation of concerns

NEXT IN RAG PIPELINE:
â†’ Data loading (how to get documents)
â†’ Text splitting (chunking for embeddings)
â†’ Embeddings (convert text to vectors)
â†’ Vector stores (store and search embeddings)
â†’ Retrieval strategies (find relevant docs)
â†’ Putting it all together (complete RAG system)
`);